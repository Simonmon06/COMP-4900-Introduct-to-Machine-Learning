{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Native Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/guzeye/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/guzeye/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized Done!!!!\n",
      "In different K(6) fold:\n",
      "(Train set)The Accuracy for  is  0.8345249196974407\n",
      "(Validation set)The Accuracy for  is  0.7757638529259451\n",
      "(Train set)The Accuracy for  is  0.8347321521085898\n",
      "(Validation set)The Accuracy for  is  0.7897462454686691\n",
      "(Train set)The Accuracy for  is  0.8394115209283051\n",
      "(Validation set)The Accuracy for  is  0.7917098445595855\n",
      "(Train set)The Accuracy for  is  0.8319519270617488\n",
      "(Validation set)The Accuracy for  is  0.794818652849741\n",
      "(Train set)The Accuracy for  is  0.8360961458765023\n",
      "(Validation set)The Accuracy for  is  0.778238341968912\n",
      "(Train set)The Accuracy for  is  0.8403439701616245\n",
      "(Validation set)The Accuracy for  is  0.7756476683937824\n",
      "The Average of 6 fold\n",
      "(Train set)The Accuracy for  is  0.8361767726390351\n",
      "(Validation set)The Accuracy for  is  0.7843207676944391 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "\n",
    "class NBModel:\n",
    "    def __init__(self):\n",
    "        self.theta_j_1=None\n",
    "        self.prob=None\n",
    "        self.num_feature=0\n",
    "        self.theta_k = [None] * 8\n",
    "        self.theta_j_k = [None] * 8\n",
    "        self.class_prob = [None] * 8\n",
    "\n",
    "    def preProcess(self,train,test):\n",
    "        vectorizer = CountVectorizer(binary=True,max_features=5000)\n",
    "        vectorizer.fit(train)\n",
    "        trainVec = vectorizer.transform(train).toarray()\n",
    "        testVec = vectorizer.transform(test).toarray()\n",
    "        self.num_feature = len(vectorizer.get_feature_names())\n",
    "        return trainVec, testVec\n",
    "    def preProcess2(self,test):\n",
    "        vectorizer = CountVectorizer(binary=True,max_features=5000)\n",
    "        vectorizer.fit(test)\n",
    "        testV = vectorizer.transform(test).toarray()\n",
    "        self.num_feature = len(vectorizer.get_feature_names())\n",
    "        return testV\n",
    "\n",
    "\n",
    "    def fit(self,x,y,k):\n",
    "        theta_1 = (y == k).sum() / float(y.shape[0])\n",
    "        theta_0 = (y != k).sum() / float(y.shape[0])\n",
    "        self.prob=np.log((theta_1/theta_0))\n",
    "        self.theta_j_1 = np.zeros(self.num_feature)\n",
    "        predict_label = np.zeros(x.shape[0])\n",
    "        self.class_prob[k-1] = theta_1/theta_0\n",
    "        self.theta_k[k-1] = theta_1\n",
    "\n",
    "        p0Num = np.ones(x.shape[1])\n",
    "        p1Num = np.ones(x.shape[1])\n",
    "\n",
    "        for j in range(self.num_feature):\n",
    "            p1Num[j]+=np.sum(x[y == k,j])\n",
    "\n",
    "        # Laplace smoothing\n",
    "        self.theta_j_1[:] = p1Num[:]/ float((y == k).sum()+2)\n",
    "        self.theta_j_k[k-1] = self.theta_j_1[:]\n",
    "\n",
    "    def predict(self,x):\n",
    "        llh_k = [None] * 8\n",
    "        llh=np.zeros(x.shape[0])\n",
    "        predict=[None]*(x.shape[0])\n",
    "\n",
    "        for k in range(0,8):\n",
    "            llh[:] = np.sum(x[:, :] * np.log(self.theta_j_k[k][:]) + (1 - x[:, :]) * np.log(\n",
    "                (1 - self.theta_j_k[k][:])),axis=1)\n",
    "            llh[:]+=self.class_prob[k]\n",
    "            llh_k[k] = llh[:]\n",
    "            llh=np.zeros(x.shape[0])\n",
    "        return np.argmax(llh_k, axis=0) + 1\n",
    "\n",
    "def Accu_eval2(y, predict):\n",
    "    count = 0\n",
    "    for i in range(0, len(y)):\n",
    "        if predict[i] ==y[i]:\n",
    "            count+=1\n",
    "    # print(\"The Accu is: \",accuracy)\n",
    "    return count/len(y)\n",
    "\n",
    "data = pd.read_csv('train.csv',dtype=str)\n",
    "test = pd.read_csv('test.csv',dtype=str)\n",
    "data = data.values\n",
    "test = test.values\n",
    "np.random.shuffle(data)\n",
    "body = data[:, 0]\n",
    "subReddit = data[:, 1]\n",
    "testbody=test[:,1]\n",
    "testIndex=test[:,0]\n",
    "\n",
    "def check(word):\n",
    "    word= word.lower()\n",
    "    if bool(re.search(r'\\d', word)) or bool(re.match(r'[^\\w]', word)):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmatized = [None]*body.shape[0]\n",
    "test_lemmatized = [None]*testbody.shape[0]\n",
    "i = 0\n",
    "for s in testbody:\n",
    "    test_lemmatized[i] = \" \".join([wnl.lemmatize(word) for word in word_tokenize(s) if (check(word))])\n",
    "    i+=1\n",
    "test_lemmatized=np.array(test_lemmatized)\n",
    "i=0\n",
    "for s in body:\n",
    "    lemmatized[i] = \" \".join([wnl.lemmatize(word) for word in word_tokenize(s) if (check(word))])\n",
    "    i+=1\n",
    "def result(re,fo,k):\n",
    "    for i in range(1,len(fo)):\n",
    "        if(fo[i]==1):\n",
    "            re[i]=k\n",
    "    return re\n",
    "lemmatized=np.array(lemmatized)\n",
    "print(\"lemmatized Done!!!!\")\n",
    "\n",
    "nb=NBModel()\n",
    "\n",
    "#k fold\n",
    "kf = KFold(n_splits=6, shuffle=False)\n",
    "def toint(x):\n",
    "    i=0\n",
    "    if x=='rpg':\n",
    "        i=1\n",
    "    if x=='anime':\n",
    "        i=2\n",
    "    if x=='datascience':\n",
    "        i=3\n",
    "    if x==\"hardware\":\n",
    "        i=4\n",
    "    if x==\"cars\":\n",
    "        i=5\n",
    "    if x==\"gamernews\":\n",
    "        i=6\n",
    "    if x==\"gamedev\":\n",
    "        i=7\n",
    "    if x=='computers':\n",
    "        i=8\n",
    "    return i\n",
    "def toString(x):\n",
    "    if x == 1:\n",
    "        return 'rpg'\n",
    "    elif x == 2:\n",
    "        return 'anime'\n",
    "    elif x == 3:\n",
    "        return 'datascience'\n",
    "    elif x == 4:\n",
    "        return \"hardware\"\n",
    "    elif x == 5:\n",
    "        return \"cars\"\n",
    "    elif x == 6:\n",
    "        return \"gamernews\"\n",
    "    elif x == 7:\n",
    "        return \"gamedev\"\n",
    "    elif x == 8:\n",
    "        return 'computers'\n",
    "    else:\n",
    "        return 'None'\n",
    "t=data\n",
    "\n",
    "result = [None]*testIndex.shape[0]\n",
    "for c in range(0,len(t[:,1])):\n",
    "    t[c,1]=toint(t[c,1])\n",
    "\n",
    "temp=t[:,1]\n",
    "AvgAccuV=0\n",
    "AvgAccuT=0\n",
    "print(\"In different K(6) fold:\")\n",
    "for train_index, test_index in kf.split(lemmatized):\n",
    "    trainx = lemmatized[train_index]\n",
    "    trainy =temp[train_index]\n",
    "    valix = lemmatized[test_index]\n",
    "    valiy = temp[test_index]\n",
    "    train, vali = nb.preProcess(trainx, valix)\n",
    "    for k in range(1,9):\n",
    "        nb.fit(train,trainy,k)\n",
    "    predictV = nb.predict(vali)\n",
    "    predictT = nb.predict(train)\n",
    "    AvgAccuV +=Accu_eval2(valiy,predictV)\n",
    "    AvgAccuT += Accu_eval2(trainy,predictT)\n",
    "    print(\"(Train set)The Accuracy for  is \", Accu_eval2(trainy,predictT))\n",
    "    print(\"(Validation set)The Accuracy for  is \", Accu_eval2(valiy,predictV))\n",
    "\n",
    "\n",
    "print(\"The Average of 6 fold\")\n",
    "print(\"(Train set)The Accuracy for  is \", AvgAccuT/6)\n",
    "print(\"(Validation set)The Accuracy for  is \", AvgAccuV/6,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with different classifier with kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Overall vali Accu: 0.8364712498965743\n",
      "LogisticRegression Overall train Accu 0.9553401866452556\n",
      "DecisionTree Overall vali Accu: 0.6581783438920705\n",
      "DecisionTree Overall train Accu 0.9998273163882511\n",
      "SGD Overall vali Accu: 0.8868074299519284\n",
      "SGD Overall train Accu 0.9975393004664326\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download(\"popular\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import gc\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, SelectFromModel\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "data = pd.read_csv('train.csv',dtype=str)\n",
    "test = pd.read_csv('test.csv',dtype=str)\n",
    "data = data.values\n",
    "test = test.values\n",
    "np.random.shuffle(data)\n",
    "body = data[:, 0]\n",
    "subReddit = data[:, 1]\n",
    "testbody=test[:,1]\n",
    "testIndex=test[:,0]\n",
    "\n",
    "def check(word):\n",
    "    word= word.lower()\n",
    "    if bool(re.search(r'\\d', word)) or bool(re.match(r'[^\\w]', word)):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmatized = [None]*body.shape[0]\n",
    "test_lemmatized = [None]*testbody.shape[0]\n",
    "i = 0\n",
    "for s in testbody:\n",
    "    test_lemmatized[i] = \" \".join([wnl.lemmatize(word) for word in word_tokenize(s) if (check(word))])\n",
    "    i+=1\n",
    "test_lemmatized=np.array(test_lemmatized)\n",
    "i=0\n",
    "for s in body:\n",
    "    lemmatized[i] = \" \".join([wnl.lemmatize(word) for word in word_tokenize(s) if (check(word))])\n",
    "    i+=1\n",
    "lemmatized=np.array(lemmatized)\n",
    "\n",
    "#ngram_range=(1, 2) \"I love it\" -> 'I' 'love' 'it' && 'I love' 'love it' \n",
    "#max_df ignore the words that the document frequency higher then x when building the voab list\n",
    "#sublinear_tf use 1+log(tf) instead of tf\n",
    "\n",
    "\n",
    "text_classiferLR =Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1, 2),max_df=0.9, sublinear_tf=True)),\n",
    "                      ('feature_selection', SelectPercentile(f_classif, percentile=40)),\n",
    "                      ('Logistic', LogisticRegression(max_iter=2000))])\n",
    "\n",
    "text_classiferDecisionTree = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1, 2),max_df=0.9, sublinear_tf=True)),\n",
    "                      ('feature_selection', SelectPercentile(f_classif, percentile=40)),\n",
    "                      ('DT', DecisionTreeClassifier())])\n",
    "\n",
    "text_classiferSGD =Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1, 2),max_df=0.9, sublinear_tf=True)),\n",
    "                      ('feature_selection', SelectPercentile(f_classif, percentile=40)),\n",
    "                      ('SGD', SGDClassifier())])\n",
    "# k fold with default parameter\n",
    "kfCValidator = KFold(n_splits=5, shuffle=True,random_state=None)\n",
    "def predictFactory(trainx,trainy,validationx,validationy,classifer,totalAccuaryV,totalAccuaryT):\n",
    "    totalAccuaryV =0\n",
    "    totalAccuaryT =0\n",
    "    #fit\n",
    "    classifer.fit(trainx,trainy)\n",
    "    #predict\n",
    "    predictV = classifer.predict(validationx)\n",
    "    predictT = classifer.predict(trainx)\n",
    "    #sum the accuray for each fold\n",
    "    totalAccuaryV += np.mean(predictV == validationy)\n",
    "    totalAccuaryT += np.mean(predictT == trainy)\n",
    "    \n",
    "    return totalAccuaryV, totalAccuaryT\n",
    "    \n",
    "    \n",
    "\n",
    "totalAccuaryLRVali =0\n",
    "totalAccuaryLRTrain =0    \n",
    "totalAccuaryDecisionTreeVali =0\n",
    "totalAccuaryDecisionTreeTrain =0 \n",
    "totalAccuarySGDVali =0\n",
    "totalAccuarySGDTrain =0 \n",
    "for train_yield, test_yield in kfCValidator.split(lemmatized):\n",
    "    trainx = lemmatized[train_yield]\n",
    "    trainy = subReddit[train_yield]\n",
    "    validationx = lemmatized[test_yield]\n",
    "    validationy = subReddit[test_yield]\n",
    "    \n",
    "    addLRV, addLRT = predictFactory(trainx,trainy,validationx,validationy,text_classiferLR,totalAccuaryLRVali,totalAccuaryLRTrain);\n",
    "    addDTV, addDTT = predictFactory(trainx,trainy,validationx,validationy,text_classiferDecisionTree,totalAccuaryLRVali,totalAccuaryLRTrain);\n",
    "    addSGDV,addSGDT= predictFactory(trainx,trainy,validationx,validationy,text_classiferSGD,totalAccuaryLRVali,totalAccuaryLRTrain);\n",
    "    totalAccuaryLRVali += addLRV\n",
    "    totalAccuaryLRTrain += addLRT\n",
    "    totalAccuaryDecisionTreeVali +=addDTV\n",
    "    totalAccuaryDecisionTreeTrain +=addDTT\n",
    "    totalAccuarySGDVali +=addSGDV\n",
    "    totalAccuarySGDTrain+=addSGDT\n",
    "\n",
    "\n",
    "LR_validation = totalAccuaryLRVali / 5.0\n",
    "LR_train = totalAccuaryLRTrain / 5.0\n",
    "\n",
    "DC_validation = totalAccuaryDecisionTreeVali/5.0\n",
    "DC_train = totalAccuaryDecisionTreeTrain/5.0\n",
    "\n",
    "SGD_validation = totalAccuarySGDVali/5.0\n",
    "SGD_Train = totalAccuarySGDTrain/5.0\n",
    "print(\"LogisticRegression Overall vali Accu:\",LR_validation )\n",
    "print(\"LogisticRegression Overall train Accu\", LR_train)\n",
    "\n",
    "print(\"DecisionTree Overall vali Accu:\",DC_validation )\n",
    "print(\"DecisionTree Overall train Accu\", DC_train)\n",
    "\n",
    "\n",
    "print(\"SGD Overall vali Accu:\",SGD_validation )\n",
    "print(\"SGD Overall train Accu\", SGD_Train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate csv file for submission, tested by using test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download(\"popular\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import gc\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, SelectFromModel\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "data = pd.read_csv('train.csv',dtype=str)\n",
    "test = pd.read_csv('test.csv',dtype=str)\n",
    "data = np.array(data)\n",
    "test = np.array(test)\n",
    "\n",
    "np.random.shuffle(data)\n",
    "# body = data[:, 0]\n",
    "\n",
    "body = (data[:, 0]).flatten()\n",
    "subReddit = data[:, 1]\n",
    "testbody = test[:, 1].flatten()\n",
    "# testbody=test[:,1]\n",
    "testIndex=test[:,0]\n",
    "\n",
    "\n",
    "\n",
    "def check(word):\n",
    "    word= word.lower()\n",
    "    if bool(re.search(r'\\d', word)) or bool(re.match(r'[^\\w]', word)):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmatized = [None]*body.shape[0]\n",
    "test_lemmatized = [None]*testbody.shape[0]\n",
    "i = 0\n",
    "for s in testbody:\n",
    "    test_lemmatized[i] = \" \".join([wnl.lemmatize(word) for word in word_tokenize(s) if (check(word))])\n",
    "    i+=1\n",
    "test_lemmatized=np.array(test_lemmatized)\n",
    "i=0\n",
    "for s in body:\n",
    "    lemmatized[i] = \" \".join([wnl.lemmatize(word) for word in word_tokenize(s) if (check(word))])\n",
    "    i+=1\n",
    "lemmatized=np.array(lemmatized)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text_classifer=Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1, 2),stop_words={'english'},max_df=0.9,sublinear_tf=True)),\n",
    "                      ('feature_selection', SelectPercentile(f_classif, percentile=20)),\n",
    "                      ('clf', SGDClassifier())])\n",
    "\n",
    "\n",
    "text_classifer.fit(lemmatized,subReddit)\n",
    "predict_array = text_classifer.predict(test_lemmatized)\n",
    "predict_array = np.reshape(predict_array, (len(predict_array), 1))\n",
    "pd.DataFrame(predict_array).to_csv(\"result.csv\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
